{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders\n",
    "In this notebook we will develop a simple autoencoder in pytorch.<br>\n",
    "Autoencoders fall under the unsupervised family, and thier purpose is to learn an encoding of the \n",
    "data, where the immediate use case is dimensionality reduction.<br>\n",
    "Autencoders where first purposed by [Rumelhart](https://apps.dtic.mil/docs/citations/ADA164453) under Hinton supervision. <br><br>\n",
    "\n",
    "## Intution\n",
    "The best way to start explaining an autoencoder is by using the following visualization: \n",
    "![alt text](img/cells.png \"Title\")\n",
    "\n",
    "The first thing we notice is that the input and output layer are the same. This is because our \"goal\"\n",
    "is that the input and the output will be exactly the same!<br>\n",
    "The second thing we notice is that we have a hidden layer with a lower dimensionality than the input and output layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:yoav]",
   "language": "python",
   "name": "conda-env-yoav-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
