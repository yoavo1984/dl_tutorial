{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classyfing names with Character Level RNN\n",
    "In this notebook we will use pytorch to classify names using character level RNN.<br>\n",
    "The puropose of this tutorial is to get familiar with simple RNN architecture \n",
    "and it's implementation in \n",
    "pytorch.<br>\n",
    "This tutorial is based on : https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "import unicodedata\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prepration\n",
    "\n",
    "### Data Utils\n",
    "In the next cell we will define some variables and function which will help\n",
    "process the text. <br>\n",
    "You can find the data here : [DATA](https://download.pytorch.org/tutorial/data.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/names/Irish.txt', 'data/names/French.txt', 'data/names/Korean.txt', 'data/names/Japanese.txt', 'data/names/Portuguese.txt', 'data/names/Italian.txt', 'data/names/Chinese.txt', 'data/names/Russian.txt', 'data/names/Polish.txt', 'data/names/German.txt', 'data/names/Scottish.txt', 'data/names/English.txt', 'data/names/Spanish.txt', 'data/names/Dutch.txt', 'data/names/Arabic.txt', 'data/names/Vietnamese.txt', 'data/names/Czech.txt', 'data/names/Greek.txt']\n"
     ]
    }
   ],
   "source": [
    "def find_files(path): return glob(path)\n",
    "\n",
    "print(find_files('data/names/*.txt'))\n",
    "\n",
    "all_letters = string.ascii_letters + \".,;\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "def read_lines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicode_to_ascii(line) for line in lines]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "We will store the data in a simple python dictionary. <br>\n",
    "The language will serve as the key, and the value of each language is all the names in that language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_lines = {} # A dictionay with key=language, value=lines of names in the language.\n",
    "all_categories = []\n",
    "\n",
    "for filename in find_files('data/names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0] # Extrach the language name.\n",
    "    all_categories.append(category)\n",
    "    lines = read_lines(filename)\n",
    "    category_lines[category] = lines\n",
    "    \n",
    "n_categories = len(all_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see the data makes sense. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ababko', 'Abaev', 'Abagyan', 'Abaidulin', 'Abaidullin'] Russian\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(4300)\n",
    "random_language = np.random.choice(all_categories) # Pick a random language\n",
    "print(category_lines[random_language][:5], random_language) # Print 5 names from the language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors Conversion\n",
    "The next thing we want to do is convert our data into the data.<br>\n",
    "Remeber this is a char based RNN so we need to find a representation for each letter.<br>\n",
    "As we are trainig a deep learining model with pytorch we will convert the data into tensors, and more specifically a one hot encdoing tensors. So each letter turns into into a vector of size n_letters with zero everywhere except the index of the letter.<br>\n",
    "Pytorch models expect the following structure as input : [batch_size, channels, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Util function to find the letter index.\n",
    "def letter_to_index(l):\n",
    "    return all_letters.find(l)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:yoav]",
   "language": "python",
   "name": "conda-env-yoav-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
